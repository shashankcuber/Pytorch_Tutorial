{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNAglEHOyncX9PpPu0AVAGO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"baR-0h3Vt1eK","executionInfo":{"status":"ok","timestamp":1668033016303,"user_tz":420,"elapsed":2345,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}}},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"code","source":["# 1d tensor with 3 elements\n","x = torch.empty(3)\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h36N7cq8t9bE","executionInfo":{"status":"ok","timestamp":1668019671724,"user_tz":420,"elapsed":6,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"38abcb8d-4305-4f5c-d0c6-58a983811fa5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2.0500e-35, 0.0000e+00, 5.0447e-44])\n"]}]},{"cell_type":"code","source":["# 2d tensor\n","x = torch.empty(3, 2)\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"unu_zq0puA_T","executionInfo":{"status":"ok","timestamp":1668019703172,"user_tz":420,"elapsed":3,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"2a34c2c0-3d42-4e06-fd65-7cd09e6820d3"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2.0500e-35, 0.0000e+00],\n","        [3.3631e-44, 0.0000e+00],\n","        [       nan, 0.0000e+00]])\n"]}]},{"cell_type":"code","source":["#creating tensor with random values\n","x = torch.randn(2,2)\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t3oVXfgBuMrN","executionInfo":{"status":"ok","timestamp":1668019750156,"user_tz":420,"elapsed":300,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"3a753965-789e-4b56-e97a-8f3ea17d82a2"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.7261,  0.4863],\n","        [-0.3610, -0.2736]])\n"]}]},{"cell_type":"code","source":["x = torch.zeros(2,2)\n","print(x)\n","print(x.dtype)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rTaj_kF5uTf_","executionInfo":{"status":"ok","timestamp":1668019793971,"user_tz":420,"elapsed":274,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"4f617437-739d-4ae4-f6e5-be1763854533"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0.],\n","        [0., 0.]])\n","torch.float32\n"]}]},{"cell_type":"code","source":["# we can make changes to the dtype of tensors in pytorch\n","x = torch.ones(2,2, dtype=torch.int)\n","print(x)\n","print(x.dtype)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QPtnVXDaucZC","executionInfo":{"status":"ok","timestamp":1668019850758,"user_tz":420,"elapsed":283,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"17215288-143a-4705-f825-3c8b3ff3dd11"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 1],\n","        [1, 1]], dtype=torch.int32)\n","torch.int32\n"]}]},{"cell_type":"code","source":["#Basic operations\n","\n","x = torch.rand(2,2)\n","y = torch.rand(2,2)\n","\n","z = x+y\n","\n","print(z)\n","z = torch.mul(x,y)\n","print(z)\n","\n","#inplace operation in _\n","\n","y.add_(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0lNkrcnKuwrG","executionInfo":{"status":"ok","timestamp":1668020422361,"user_tz":420,"elapsed":390,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"e3ed79fa-2e7a-4850-b9b5-644b6ad050b7"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.0905, 1.2392],\n","        [0.9625, 1.1074]])\n","tensor([[0.1178, 0.3583],\n","        [0.1598, 0.2638]])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0905, 1.2392],\n","        [0.9625, 1.1074]])"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["#view\n","\n","x = torch.rand(4,4)\n","print(x)\n","y = x.view(-1, 8)\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JMeSfxSwwjOu","executionInfo":{"status":"ok","timestamp":1668020641472,"user_tz":420,"elapsed":3,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"d2f0d175-b5a8-4918-f70d-1d8548b03cbc"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1569, 0.5569, 0.6483, 0.3967],\n","        [0.3442, 0.7636, 0.2079, 0.1969],\n","        [0.1392, 0.7447, 0.4946, 0.8619],\n","        [0.2081, 0.2300, 0.3563, 0.7139]])\n","tensor([[0.1569, 0.5569, 0.6483, 0.3967, 0.3442, 0.7636, 0.2079, 0.1969],\n","        [0.1392, 0.7447, 0.4946, 0.8619, 0.2081, 0.2300, 0.3563, 0.7139]])\n"]}]},{"cell_type":"code","source":["# numpy->tensor and vice versa\n","\n","#Things to keep in mind tensor runs on gpu and normal nd array runs on cpu ,\n","#so if they share the same location then doing operation on one will effect the other same as call by reference concept\n","\n","import numpy as np\n","\n","a = torch.ones(2)\n","b = a.numpy()\n","print(b)\n","\n","a = torch.from_numpy(b)\n","print(a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BkmSKb-TxooL","executionInfo":{"status":"ok","timestamp":1668020850147,"user_tz":420,"elapsed":2,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"030f3baf-7acb-488d-8798-da6254b0b995"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["[1. 1.]\n","tensor([1., 1.])\n"]}]},{"cell_type":"code","source":["#cuda to create tensor on gpu\n","\n","# if torch.cuda.is_available():\n","#   device = torch.device(\"cuda\")\n","#   x = torch.ones(5, device=device)\n","#   y = torch.ones(5)\n","#   y = y.to(device)\n","#   z = x + y\n","#   z.numpy() # this will create error as numpy array runs on cpu and not gpu so we have to move it to cpu\n","#   z = z.to(\"cpu\")\n"],"metadata":{"id":"iLGOKYV_yHMe","executionInfo":{"status":"ok","timestamp":1668021196750,"user_tz":420,"elapsed":5,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# requires_grad to True just let's it knows that later we might have to calculate gradient of this tensor\n","\n","x = torch.ones(5, requires_grad=True)\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uAFUtL83z5R_","executionInfo":{"status":"ok","timestamp":1668021257772,"user_tz":420,"elapsed":7,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"00fa1219-8654-4959-e25f-ff3d0f837831"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["# Autograd for optmimzation to calculate gradient in pytorch"],"metadata":{"id":"RcDZsq480-15"}},{"cell_type":"code","source":["x = torch.randn(5)\n","print(x)\n","#now suppose we want to calculate gradient of some function wrt x then we have to specify in it's argument required_grad= True\n","\n","x = torch.randn(3, requires_grad=True)\n","print(x)\n","\n","#now if we perform any operation on this tensor\n","\n","y = x + 2\n","\n","\"\"\"\n","x\n","    +.     y \n","2\n","\n","In back propogation it will calculate dy/dx  and is added backward \n","\"\"\"\n","\n","print(y)\n","\n","\n","z = y * y * 2\n","print(z)\n","\n","# z = z.mean()\n","# print(z)\n","\n","z.backward() # dz/dx\n","\n","print(x.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":461},"id":"3lhXfBxM1DEF","executionInfo":{"status":"error","timestamp":1668022313458,"user_tz":420,"elapsed":7,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"e04b5875-3620-44a2-f976-dca541a369ad"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 1.0807, -1.6904,  0.0497, -0.4585, -0.8382])\n","tensor([ 0.4099,  0.1375, -0.4269], requires_grad=True)\n","tensor([2.4099, 2.1375, 1.5731], grad_fn=<AddBackward0>)\n","tensor([11.6154,  9.1382,  4.9490], grad_fn=<MulBackward0>)\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-9d20901a6996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# print(z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# dz/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"]}]},{"cell_type":"code","source":["# to resolve this error \n","# z is a scaler value so we don't need any vector but if z is a vector \n","#then to apply backward we need to have vector of same size as it does jacobian multiplication\n","\n","v = torch.tensor([0.1, 1.0, 0.001], dtype = torch.float32)\n","\n","z.backward(v)\n","print(x.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUO1uKfT1ILC","executionInfo":{"status":"ok","timestamp":1668022368355,"user_tz":420,"elapsed":3,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"19cc641d-b269-4822-eb48-10daa44e0844"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([9.6397e-01, 8.5502e+00, 6.2922e-03])\n"]}]},{"cell_type":"code","source":["# prevent pytorch from tracking the history of gradient\n","\"\"\"\n","option 1:x.requires_grad_(False)\n","\n","option 2: x.detach()\n","\n","option 3: with torch.no_grad()\n","\"\"\"\n","x = torch.randn(3, requires_grad = True)\n","#print(x)\n","# x.requires_grad_(False)\n","\n","# y = x.detach()\n","# print(y)\n","\n","y = x + 2\n","print(y)\n","\n","with torch.no_grad():\n","  y = x + 2\n","\n","print(y) \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gtI4byPf4XaL","executionInfo":{"status":"ok","timestamp":1668022784393,"user_tz":420,"elapsed":4,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"825268a4-8d40-44a4-89c4-3139fc02cb49"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2.5563, 2.4875, 2.9160], grad_fn=<AddBackward0>)\n","tensor([2.5563, 2.4875, 2.9160])\n"]}]},{"cell_type":"code","source":["# there is an issue of accumulation of gradient  during training step let's see\n","\n","weights = torch.ones(4, requires_grad=True)\n","\n","for epoch in range(3):\n","  model_output = (weights*3).sum()\n","  model_output.backward()\n","\n","  print(weights.grad)\n","  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oQkLYkbI4_db","executionInfo":{"status":"ok","timestamp":1668023553287,"user_tz":420,"elapsed":341,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"ade6975d-f43e-4696-e1fc-cdb2c2024c5a"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([3., 3., 3., 3.])\n","tensor([6., 6., 6., 6.])\n","tensor([9., 9., 9., 9.])\n"]}]},{"cell_type":"code","source":["# Above weights are getting accumulated and making our dataset hampered\n","# to resolve this in every epoch make the gradients zero\n","\n","weights = torch.ones(4, requires_grad=True)\n","for epoch in range(3):\n","  model_output = (weights*3).sum()\n","  model_output.backward()\n","\n","  print(weights.grad)\n","\n","  weights.grad.zero_()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GfYQSmuE84Y7","executionInfo":{"status":"ok","timestamp":1668023703982,"user_tz":420,"elapsed":4,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"c7b1d41e-9bdf-4233-e01d-a6edbc092c8e"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n"]}]},{"cell_type":"code","source":["# pytorch inbuilt optimizers\n","\n","\"\"\"\n","\n","weights = torch.ones(4, requires_grad=True)\n","\n","optimizer = torch.optim.SGD(weights, lr=0.01)\n","\n","optimizer.step()\n","\n","optimizer.zero_grad()\n","\n"," Same thing should be done with inbuild optimizers as well\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"Fx_ebJfi9WjN","executionInfo":{"status":"ok","timestamp":1668024161057,"user_tz":420,"elapsed":7,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"3967f970-55b4-48e3-87a3-53e1f8dc06f3"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\nweights = torch.ones(4, requires_grad=True)\\n\\noptimizer = torch.optim.SGD(weights, lr=0.01)\\n\\noptimizer.step()\\n\\noptimizer.zero_grad()\\n\\n Same thing should be done with inbuild optimizers as well\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["\"\"\"\n","Backpropogation\n","\n","x -> a(x) -> b(y) -> z\n","\n","to calculate dz/dx we apply chain rule i.e dz/dy * dy/dx\n","\n","kind of calculating gradient from back to front to get the final gradient we are interested in\n","\n","\n","######### COMPUTATIONAL GRAPH #########\n","\n","x\n","    (f = x * y) -------> z\n","y\n","\n","here we want to compute local gradients.  dz/dy = d(x*y)/ dy = x and dz/dx = d(x*y)/ dx = y\n","\n","\n","3 step process:\n","\n","1. Forward Pass\n","2. Compute local gradients\n","3. Do backpropogation to update the weights\n","\n","\"\"\"\n","\n","\n","x = torch.tensor(1.0)\n","y= torch.tensor(2.0)\n","\n","w = torch.tensor(1.0, requires_grad = True)\n","\n","# forward pass to compute the loss\n","\n","y_hat = w*x\n","\n","loss = (y_hat - y)**2\n","\n","print(loss)\n","\n","\n","#backward pass\n","\n","#whole gradient computation\n","loss.backward()\n","\n","print(w.grad)\n","\n","\n","## update the weights and again do forward and backward pass"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U5msHDCh-6aa","executionInfo":{"status":"ok","timestamp":1668025192409,"user_tz":420,"elapsed":394,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"3e583671-edf5-41e7-e3a9-8085431b1009"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1., grad_fn=<PowBackward0>)\n","tensor(-2.)\n"]}]},{"cell_type":"code","source":["#gradient descent using autograd\n","x = np.array([1,2,3,4], dtype=np.float32)\n","y = np.array([2,4,6,8], dtype=np.float32)\n","\n","w = 0.0\n","\n","# model prediction\n","def forward(x):\n","  return w*x\n","\n","# loss MSE\n","def loss(y, y_predicted):\n","  return ((y_predicted-y)**2).mean()\n","\n","# gradient\n","# MSE = 1/N *(W*x-y)**2\n","# dJ/dw = 1/N 2x(w*x-y)\n","\n","def gradient(x, y, y_predicted):\n","  return np.dot(2*x, y_predicted-y).mean()\n","\n","\n","\n","print(f'Prediction before training: f(5) = {forward(5):.3f}')\n","\n","\n","#training\n","\n","lr = 0.01\n","n_iters = 14\n","\n","for epoch in range(n_iters):\n","  #prediction\n","  y_pred = forward(x)\n","\n","  #loss \n","  l = loss(y, y_pred)\n","\n","  #gradient\n","  dw = gradient(x, y, y_pred)\n","\n","  #update weights\n","\n","  w = w - lr*dw\n","\n","  if epoch % 1==0:\n","    print(f'epoch{epoch+1}: w={w:.3f}, loss={l:.8f}')\n","\n","print(f'Prediction after training: f(5) = {forward(5):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xCag-o6hC8Fs","executionInfo":{"status":"ok","timestamp":1668033662188,"user_tz":420,"elapsed":2,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"8ff722da-69c0-4437-dea4-1d0561f7f6ee"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction before training: f(5) = 0.000\n","epoch1: w=1.200, loss=30.00000000\n","epoch2: w=1.680, loss=4.79999924\n","epoch3: w=1.872, loss=0.76800019\n","epoch4: w=1.949, loss=0.12288000\n","epoch5: w=1.980, loss=0.01966083\n","epoch6: w=1.992, loss=0.00314574\n","epoch7: w=1.997, loss=0.00050331\n","epoch8: w=1.999, loss=0.00008053\n","epoch9: w=1.999, loss=0.00001288\n","epoch10: w=2.000, loss=0.00000206\n","epoch11: w=2.000, loss=0.00000033\n","epoch12: w=2.000, loss=0.00000005\n","epoch13: w=2.000, loss=0.00000001\n","epoch14: w=2.000, loss=0.00000000\n","Prediction after training: f(5) = 10.000\n"]}]},{"cell_type":"code","source":["# doing same thing with pytorch\n","#gradient descent using autograd\n","x = torch.tensor([1,2,3,4], dtype=torch.float32)\n","y = torch.tensor([2,4,6,8], dtype=torch.float32)\n","\n","w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n","\n","\n","# model prediction\n","def forward(x):\n","  return w*x\n","\n","# loss MSE\n","def loss(y, y_predicted):\n","  return ((y_predicted-y)**2).mean()\n","\n","# gradient\n","# MSE = 1/N *(W*x-y)**2\n","# dJ/dw = 1/N 2x(w*x-y)\n","\n","# def gradient(x, y, y_predicted):\n","#   return np.dot(2*x, y_predicted-y).mean()\n","\n","\n","\n","print(f'Prediction before training: f(5) = {forward(5):.3f}')\n","\n","\n","#training\n","\n","lr = 0.01\n","n_iters = 70\n","\n","\n","for epoch in range(n_iters):\n","  #prediction\n","  y_pred = forward(x)\n","\n","  #loss \n","  l = loss(y, y_pred)\n","\n","  #gradient\n","  # dw = gradient(x, y, y_pred)\n","  \n","  #pytorch for gradient\n","  l.backward() # gradient of loss wrt to w\n","\n","  #update weights\n","  with torch.no_grad():\n","    w -= lr * w.grad\n","  \n","  # empty the gradient for avoiding it's accumulation\n","  w.grad.zero_()\n","\n","  if epoch % 1==0:\n","    print(f'epoch{epoch+1}: w={w:.3f}, loss={l:.8f}')\n","\n","\n","\n","print(f'Prediction after training: f(5) = {forward(5):.3f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eoZp4OusjLjb","executionInfo":{"status":"ok","timestamp":1668035121056,"user_tz":420,"elapsed":5,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"c285933c-10fd-4e3a-d49f-9a3e11251a88"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction before training: f(5) = 0.000\n","epoch1: w=0.300, loss=30.00000000\n","epoch2: w=0.555, loss=21.67499924\n","epoch3: w=0.772, loss=15.66018772\n","epoch4: w=0.956, loss=11.31448650\n","epoch5: w=1.113, loss=8.17471695\n","epoch6: w=1.246, loss=5.90623236\n","epoch7: w=1.359, loss=4.26725292\n","epoch8: w=1.455, loss=3.08308983\n","epoch9: w=1.537, loss=2.22753215\n","epoch10: w=1.606, loss=1.60939169\n","epoch11: w=1.665, loss=1.16278565\n","epoch12: w=1.716, loss=0.84011245\n","epoch13: w=1.758, loss=0.60698116\n","epoch14: w=1.794, loss=0.43854395\n","epoch15: w=1.825, loss=0.31684780\n","epoch16: w=1.851, loss=0.22892261\n","epoch17: w=1.874, loss=0.16539653\n","epoch18: w=1.893, loss=0.11949898\n","epoch19: w=1.909, loss=0.08633806\n","epoch20: w=1.922, loss=0.06237914\n","epoch21: w=1.934, loss=0.04506890\n","epoch22: w=1.944, loss=0.03256231\n","epoch23: w=1.952, loss=0.02352631\n","epoch24: w=1.960, loss=0.01699772\n","epoch25: w=1.966, loss=0.01228084\n","epoch26: w=1.971, loss=0.00887291\n","epoch27: w=1.975, loss=0.00641066\n","epoch28: w=1.979, loss=0.00463169\n","epoch29: w=1.982, loss=0.00334642\n","epoch30: w=1.985, loss=0.00241778\n","epoch31: w=1.987, loss=0.00174685\n","epoch32: w=1.989, loss=0.00126211\n","epoch33: w=1.991, loss=0.00091188\n","epoch34: w=1.992, loss=0.00065882\n","epoch35: w=1.993, loss=0.00047601\n","epoch36: w=1.994, loss=0.00034392\n","epoch37: w=1.995, loss=0.00024848\n","epoch38: w=1.996, loss=0.00017952\n","epoch39: w=1.996, loss=0.00012971\n","epoch40: w=1.997, loss=0.00009371\n","epoch41: w=1.997, loss=0.00006770\n","epoch42: w=1.998, loss=0.00004891\n","epoch43: w=1.998, loss=0.00003534\n","epoch44: w=1.998, loss=0.00002553\n","epoch45: w=1.999, loss=0.00001845\n","epoch46: w=1.999, loss=0.00001333\n","epoch47: w=1.999, loss=0.00000963\n","epoch48: w=1.999, loss=0.00000696\n","epoch49: w=1.999, loss=0.00000503\n","epoch50: w=1.999, loss=0.00000363\n","epoch51: w=1.999, loss=0.00000262\n","epoch52: w=2.000, loss=0.00000190\n","epoch53: w=2.000, loss=0.00000137\n","epoch54: w=2.000, loss=0.00000099\n","epoch55: w=2.000, loss=0.00000071\n","epoch56: w=2.000, loss=0.00000052\n","epoch57: w=2.000, loss=0.00000037\n","epoch58: w=2.000, loss=0.00000027\n","epoch59: w=2.000, loss=0.00000019\n","epoch60: w=2.000, loss=0.00000014\n","epoch61: w=2.000, loss=0.00000010\n","epoch62: w=2.000, loss=0.00000007\n","epoch63: w=2.000, loss=0.00000005\n","epoch64: w=2.000, loss=0.00000004\n","epoch65: w=2.000, loss=0.00000003\n","epoch66: w=2.000, loss=0.00000002\n","epoch67: w=2.000, loss=0.00000001\n","epoch68: w=2.000, loss=0.00000001\n","epoch69: w=2.000, loss=0.00000001\n","epoch70: w=2.000, loss=0.00000001\n","Prediction after training: f(5) = 10.000\n"]}]},{"cell_type":"markdown","source":["Making the whole model pipeline in pytorch\n","\n","1. Design the model (input, output_size, fwd pass)\n","2. construct loss and optimizer\n","3. training loop\n","\n","- forward pass: compute prediction\n","- backward pass : gradients\n","- update weights "],"metadata":{"id":"ZBaxqURepwJE"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","# doing same thing with pytorch\n","#gradient descent using autograd\n","x = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n","y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n","\n","# w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n","\n","\n","# model prediction\n","# def forward(x):\n","#   return w*x\n","\n","n_samples, n_features = x.shape\n","print(n_samples, n_features)\n","\n","x_test = torch.tensor([5], dtype=torch.float32)\n","input_size = n_features\n","output_size = n_features\n","\n","model = nn.Linear(input_size, output_size)\n","\n","# loss MSE\n","# def loss(y, y_predicted):\n","#   return ((y_predicted-y)**2).mean()\n","\n","loss = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","\n","# gradient\n","# MSE = 1/N *(W*x-y)**2\n","# dJ/dw = 1/N 2x(w*x-y)\n","\n","# def gradient(x, y, y_predicted):\n","#   return np.dot(2*x, y_predicted-y).mean()\n","\n","\n","\n","print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n","\n","\n","#training\n","\n","lr = 0.01\n","n_iters = 10\n","\n","\n","for epoch in range(n_iters):\n","  #prediction\n","  y_pred = forward(x)\n","\n","  #loss \n","  l = loss(y, y_pred)\n","\n","  #gradient\n","  # dw = gradient(x, y, y_pred)\n","  \n","  #pytorch for gradient\n","  l.backward() # gradient of loss wrt to w\n","\n","\n","  optimizer.step()\n","\n","  #update weights\n","  # with torch.no_grad():\n","  #   w -= lr * w.grad\n","  \n","  # empty the gradient for avoiding it's accumulation\n","  # w.grad.zero_()\n","\n","  optimizer.zero_grad()\n","\n","  if epoch % 1==0:\n","    [w,b] = model.parameters()\n","    print(f'epoch{epoch+1}: w={w[0][0].item():.3f}, loss={l:.8f}')\n","\n","\n","\n","print(f'Prediction after training: f(5) = {model(x_test).item():.3f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pAPNXesnqIzb","executionInfo":{"status":"ok","timestamp":1668036344027,"user_tz":420,"elapsed":203,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"d971bca8-0d9d-4c98-e955-9e881a425c79"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["4 1\n","Prediction before training: f(5) = 5.056\n","epoch1: w=0.950, loss=8.45141983\n","epoch2: w=1.107, loss=8.27035809\n","epoch3: w=1.241, loss=5.97533464\n","epoch4: w=1.355, loss=4.31717920\n","epoch5: w=1.452, loss=3.11916208\n","epoch6: w=1.534, loss=2.25359511\n","epoch7: w=1.604, loss=1.62822247\n","epoch8: w=1.663, loss=1.17639065\n","epoch9: w=1.714, loss=0.84994221\n","epoch10: w=1.757, loss=0.61408287\n","Prediction after training: f(5) = 9.091\n"]}]},{"cell_type":"markdown","source":["# Creating a custom model now on the given framework"],"metadata":{"id":"GoUyR23Ut2rf"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","# doing same thing with pytorch\n","\n","x = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n","y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n","\n","\n","n_samples, n_features = x.shape\n","print(n_samples, n_features)\n","\n","x_test = torch.tensor([5], dtype=torch.float32)\n","input_size = n_features\n","output_size = n_features\n","\n","#model = nn.Linear(input_size, output_size)\n","# it's more in depth custom implementation\n","class LinearRegression(nn.Module):\n","\n","  def __init__(self, input_dimension, output_dimension):\n","    super(LinearRegression, self).__init__()\n","    #define layers\n","    self.lin = nn.Linear(input_dimension, output_dimension)\n","\n","  def forward(self, x):\n","    return self.lin(x)\n","\n","model = LinearRegression(input_size, output_size)\n","\n","loss = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","\n","print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n","\n","\n","#training\n","\n","lr = 0.01\n","n_iters = 10\n","\n","\n","for epoch in range(n_iters):\n","  #prediction\n","  y_pred = forward(x)\n","\n","  #loss \n","  l = loss(y, y_pred)\n","\n","  #gradient\n","  # dw = gradient(x, y, y_pred)\n","  \n","  #pytorch for gradient\n","  l.backward() # gradient of loss wrt to w\n","\n","\n","  optimizer.step()\n","\n","  #update weights\n","  # with torch.no_grad():\n","  #   w -= lr * w.grad\n","  \n","  # empty the gradient for avoiding it's accumulation\n","  # w.grad.zero_()\n","\n","  optimizer.zero_grad()\n","\n","  if epoch % 1==0:\n","    [w,b] = model.parameters()\n","    print(f'epoch{epoch+1}: w={w[0][0].item():.3f}, loss={l:.8f}')\n","\n","\n","\n","print(f'Prediction after training: f(5) = {model(x_test).item():.3f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kuIAcVCFt8aR","executionInfo":{"status":"ok","timestamp":1668036873113,"user_tz":420,"elapsed":257,"user":{"displayName":"Shashank Pathak","userId":"01797536954556681088"}},"outputId":"cd65e78e-12e1-4ed5-b57b-09cf32af5624"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["4 1\n","Prediction before training: f(5) = 1.891\n","epoch1: w=0.560, loss=0.44367492\n","epoch2: w=0.776, loss=15.55922222\n","epoch3: w=0.959, loss=11.24153709\n","epoch4: w=1.115, loss=8.12201118\n","epoch5: w=1.248, loss=5.86815357\n","epoch6: w=1.361, loss=4.23974037\n","epoch7: w=1.457, loss=3.06321263\n","epoch8: w=1.538, loss=2.21317077\n","epoch9: w=1.608, loss=1.59901571\n","epoch10: w=1.666, loss=1.15528870\n","Prediction after training: f(5) = 7.425\n"]}]}]}